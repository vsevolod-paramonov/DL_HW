{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "lonely-delta",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Глубинное обучение 1 / Введение в глубинное обучение, ФКН ВШЭ\n",
    "\n",
    "## Домашнее задание 1. Часть 1: автоматическое дифференцирование.\n",
    "\n",
    "### Общая информация\n",
    "\n",
    "Оценка после штрафа после мягкого дедлайна вычисляется по формуле $M_{\\text{penalty}} = M_{\\text{full}} \\cdot 0.85^{t/1440}$, где $M_{\\text{full}}$ — полная оценка за работу без учета штрафа, а $t$ — время в минутах, прошедшее после мягкого дедлайна (округление до двух цифр после запятой). Таким образом, спустя первые сутки после мягкого дедлайна вы не можете получить оценку выше 8.5, а если сдать через четыре дня после мягкого дедлайна, то ваш максимум — 5.22 балла.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "Максимально допустимая оценка за работу — 10 баллов. Сдавать задание после указанного срока сдачи нельзя.\n",
    "\n",
    "Задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов. Если вы нашли решение какого-то из заданий (или его часть) в открытом источнике, необходимо указать ссылку на этот источник в отдельном блоке в конце вашей работы (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник).\n",
    "\n",
    "Неэффективная реализация кода может негативно отразиться на оценке. Также оценка может быть снижена за плохо читаемый код и плохо оформленные графики. Все ответы должны сопровождаться кодом или комментариями о том, как они были получены.\n",
    "\n",
    "### О задании\n",
    "\n",
    "В этом задании вам предстоит реализовать свой фреймворк для обучения нейронных сетей на основе `numpy`. Интерфейс фреймворка будет максимально копировать PyTorch, так что вы немного познакомитесь с тем, как все устроено изнутри. Директория `modules` содержит файлы с шаблонами фреймровка, а `tests` &mdash; тесты для проверки корректности ваших реализаций.\n",
    "\n",
    "Ячейка ниже повзоляет переподгружать питоновские модули, которые вы изменили после импорта, без необходимости перезапускать ноубук."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "suspended-death",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lonely-component",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "optimum-mechanics",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 0. Автоматическое дифференцирование\n",
    "\n",
    "Самый главным объектом в нашем фреймворке будет абстракция слоя (класс `Module`), которая реализована в файле `modules/base.py`. Перед тем как писать свой код, ознакомьтесь с реализацией класса `Module`. Каждый слой должен поддерживать две операции: проход вперед, который принимает выход предыдущего слоя и вычисляет функцию слоя, и проход назад, который принимает выход предыдущего слоя и производную по своему выходу, а возвращает производную по входу, попутно обновляя градиент по своим параметрам. Вспомним общую схему еще раз. Пусть $f(x, w)$ &mdash; это наша функция слоя, которая зависит от входа $x$ и параметров в $w$, $\\ell$ - функция потерь, градиент по параметрам которой нас интересует. Тогда:\n",
    "\n",
    "- Проход вперед:\n",
    "\n",
    "$$y = f(x, w)$$\n",
    "\n",
    "- Проход назад:\n",
    "\n",
    "$$\\frac{d\\ell}{dx} = \\frac{d\\ell}{dy} \\cdot \\frac{df(x, w)}{dx}$$\n",
    "\n",
    "$$\\frac{d\\ell}{dw} = \\frac{d\\ell}{dy} \\cdot \\frac{df(x, w)}{dw}$$\n",
    "\n",
    "Таким образом, при проходе вперед в слой передается $x$, при проходе назад $x$ и $\\frac{d\\ell}{dy}$. Кроме того, каждый слой сохраняет свой выход при проходе вперед, чтобы затем передать его в следующий слой при проходе назад. Сответственно, базовый класс `Module` реализует функции `forward` (или его alias, метод `__call__`, аналогично тому, как это сделано в PyTorch) и `backward`. Кроме того, шаблоны содержат некоторые служебные функции, в том числе `train` и `eval`, которые меняют режим слоя. Все слои, которые вам нужно реализовать, будут наследоваться от класса `Module`. В них потребуется реализовать методы `compute_output`, `compute_grad_input` и `update_grad_parameters` (если у слоя есть обучаемые параметры). За подробностями обращайтесь к док-строкам в шаблонах. Мы будем реализовывать слои аналогично соответствующим слоям из PyTorch, поэтому можете сверяться с документацией для уточнения значений параметров слоев. Для вашего удобства мы приводим тесты для отладки реализаций, ваше решение должно их проходить (баллы начисляются за пройденные тесты). В случае возникновения затруднений советуем дебажить код, сравнивая вашу реализацию с модулями из PyTorch.\n",
    "\n",
    "**Важно:** мы хотим получить такое же поведение, как у PyTorch, поэтому если сделать несколько проходов назад без вызова функции `zero_grad`, то градиенты со всех проходов назад должны суммироваться. Это значит, что в функции `update_grad_parameters` нужно прибавить новый градиент к уже имеющемуся, а не перезаписать его.\n",
    "\n",
    "Обратите внимание, что все ваши подсчеты функций и градиентов должны быть **векторизованными**, то есть включать только операции на `numpy`/`scipy` и **никаких питоновских циклов** (или оберток над ними из указанных библиотек). Специальные места, в которых циклы разрешены, будут указаны отдельно."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "traditional-vietnam",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Линейный слой (1 балл)\n",
    "\n",
    "- Прототип: [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear)\n",
    "- Расположение: `modules.layers.Linear`\n",
    "\n",
    "Отныне мы будем иметь в виду, что вход нейросети $x$ имеет размер $B \\times N$, где $B$ &mdash; размер мини-батча, а $N$ &mdash; размерность. Функция слоя выглядит как:\n",
    "\n",
    "$$\n",
    "y = x \\, W^T + b,\n",
    "$$\n",
    "\n",
    "где $W \\in \\mathbb{R}^{M \\times N}, b \\in \\mathbb{R}^M$. Таким образом, выход слоя имеет размер $B \\times M$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "75f071f4",
   "metadata": {},
   "source": [
    "> -------\n",
    "> $$ f(x, W) = xW^T + b $$\n",
    "> $$ \\frac{\\partial f(x, W)}{\\partial x} = W^T $$\n",
    "> $$ \\frac{\\partial f(x, W)}{\\partial W} = x $$\n",
    "> $$ \\frac{\\partial f(x, W)}{\\partial b} = \\vec{1} $$\n",
    "> \n",
    "> $$ \\frac{\\partial l}{\\partial x} = \\frac{\\partial l}{\\partial f(x, W)} \\times \\frac{\\partial f(x, W)}{\\partial x} = grad\\_output \\times  W^T $$\n",
    "> $$ \\frac{\\partial l}{\\partial W} = \\frac{\\partial l}{\\partial f(x, W)} \\times \\frac{\\partial f(x, W)}{\\partial W} = grad\\_output \\times  x $$\n",
    "> $$ \\frac{\\partial l}{\\partial b} = \\frac{\\partial l}{\\partial f(x, W)} \\times \\frac{\\partial f(x, W)}{\\partial b} = grad\\_output \\times  \\vec{1} $$\n",
    "> -------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hired-modem",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_linear ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_linear()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "improving-satellite",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Batch-нормализация (2.5 балла)\n",
    "\n",
    "- Прототип: [nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html#torch.nn.BatchNorm1d)\n",
    "- Расположение: `modules.layers.BatchNormalization`\n",
    "\n",
    "Batch-нормализация &mdash; первый слой, который работает по-разному в train и eval режимах.\n",
    "\n",
    "**Режим train:**\n",
    "\n",
    "1. Для каждой координаты входа считаем статистики по мини-батчу ($x_i \\in \\mathbb{R}^N$ &mdash; один объект в мини-батче):\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{B} \\sum_{i=1}^B x_i, \\quad\\quad \\mu \\in \\mathbb{R}^N \\\\\n",
    "\\sigma^2 = \\frac{1}{B} \\sum_{i=1}^B (x_i - \\mu)^2, \\quad\\quad \\sigma^2 \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "Обратите внимание, что здесь используется **смещенная** оценка дисперсии (то есть мы делим сумму квадратов отклонений на $B$, а не на $B-1$).\n",
    "\n",
    "2. Нормируем вход с учетом статистик:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "3. Применяем афинное преобразование к нормированному входу (если `affine = True`), умножение поэлементное. Это и будет выход слоя.\n",
    "\n",
    "$$\n",
    "y_i = \\hat{x}_i * w + b, \\quad\\quad w, b \\in \\mathbb{R}^N\n",
    "$$\n",
    "\n",
    "4. Обновляем бегущие статистики слоя:\n",
    "\n",
    "$$\n",
    "\\text{running\\_mean} = (1 - \\text{momentum}) \\cdot \\text{running\\_mean} + \\text{momentum} \\cdot \\mu \\\\\n",
    "\\text{running\\_var} = (1 - \\text{momentum}) \\cdot \\text{running\\_var} + \\text{momentum} \\cdot \\frac{B}{B - 1}\n",
    "\\cdot \\sigma^2\n",
    "$$\n",
    "\n",
    "Здесь перенормировка $\\sigma^2$ необходима, чтобы обновлять бегущую дисперсию **несмещенной** оценкой (точно так же это реализовано в PyTorch).\n",
    "\n",
    "К параметрам слоя, которые обновляются градиентом, относятся только $w$ (`weight`) и $b$ (`bias`), но не `running_mean` и `running_var`.\n",
    "\n",
    "**Режим eval:**\n",
    "\n",
    "1. Нормируем вход, используя бегущие статистики:\n",
    "\n",
    "$$\n",
    "\\hat{x}_i = \\frac{x_i - \\text{running\\_mean}}{\\sqrt{\\text{running\\_var} + \\varepsilon}}\n",
    "$$\n",
    "\n",
    "2. Применяем афинное преобразование к нормированному входу:\n",
    "\n",
    "$$\n",
    "y_i = \\hat{x}_i * w + b\n",
    "$$\n",
    "\n",
    "**Хозяйке на заметку**\n",
    "\n",
    "- Убедитесь, что проход назад корректно работает и для train, и для eval режимов\n",
    "- Сохраняйте промежуточные вычисления при проходе вперед, чтобы переиспользовать их при проходе назад\n",
    "- Весьма вероятно, что у вас не получится правильная реализация с первого раза. Не отчаивайтесь, автор задания тоже потратил не один час, пока этот модуль не заработал. Если чувствуете, что зашли в тупик, то пользоваться гуглом никто не запрещал, но не забудьте указать источники, которыми пользуетесь."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1169640f",
   "metadata": {},
   "source": [
    "> -------\n",
    "> $$ f(\\hat x_i, w) = \\hat x_i * w + b = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\varepsilon}} * w + b = w \\frac{x_i}{\\sqrt{\\sigma^2(x) + \\varepsilon}} - w \\frac{\\mu(x)}{\\sqrt{\\sigma^2(x) + \\varepsilon}} + b $$\n",
    "> $$ \\frac{\\partial l}{\\partial x_i} = \\frac{\\partial l}{\\partial \\hat x_i} \\frac{\\partial \\hat x_i}{\\partial x_i} + \\frac{\\partial l}{\\partial \\mu(x)} \\frac{\\partial \\mu(x)}{\\partial x_i} + \\frac{\\partial l}{\\partial \\sigma^2(x)} \\frac{\\partial \\sigma^2(x)}{\\partial x_i} $$\n",
    "> $$ \\hat x_i = f(x_i, \\mu(x), \\sigma^2(x)) $$ \n",
    "> $$ \\mu(x) = f(x_1, \\dots, x_B) $$\n",
    "> $$ \\sigma^2(x) = f(x_1, \\dots, x_B, \\mu(x)) $$\n",
    "> $$ \\frac{\\partial \\hat x_i}{\\partial x_i} = \\frac{1}{\\sqrt{\\sigma^2(x) + \\varepsilon}}  $$\n",
    "> $$ \\frac{\\partial l(l_1(\\sigma^2(x)), \\dots, l_B(\\sigma^2(x)))}{\\partial \\sigma^2(x)} = \\sum_{i=1}^{B} \\frac{\\partial l}{\\partial \\hat x_i} \\times \\frac{-1}{2} \\frac{x_i - \\mu(x)}{(\\sigma^2(x) + \\varepsilon)^{\\frac{3}{2}}} $$\n",
    "> $$ \\frac{\\partial l(l_1(\\mu(x)), \\dots, l_B(\\mu(x)), \\sigma^2(\\mu(x)))}{\\partial \\mu(x)} = \\Big[ \\sum_{i=1}^{B} \\frac{\\partial l}{\\partial \\hat x_i} \\times \\frac{-1}{\\sqrt{\\sigma^2(x) + \\varepsilon}} \\Big] - \\frac{2 \\sum_{i=1}^{B} (x_i - \\mu(x))}{B} \\frac{\\partial l}{\\partial \\sigma^2(x)} $$\n",
    "> $$ \\frac{\\partial \\sigma^2(x)}{\\partial x_i} = \\frac{2(x_i - \\mu(x))}{B} $$\n",
    "> $$ \\frac{\\partial \\mu(x)}{\\partial x_i} = \\frac{1}{B} $$\n",
    ">--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "every-prison",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_bn ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_bn()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "successful-dover",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Dropout (1 балл)\n",
    "\n",
    "- Прототип: [nn.Dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html#torch.nn.Dropout)\n",
    "- Расположение: `modules.layers.Dropout`\n",
    "\n",
    "Dropout &mdash; еще один слой, чье поведение различно в train и eval режимах. Поведения слоя регулируется параметром $p$ &mdash; вероятностью занулить координату входа.\n",
    "\n",
    "**Режим train:**\n",
    "\n",
    "Обозначим за $m$ бинарную маску, имеющую такой же размер, как вход $x$. Маска генерируется по правилу $m_{ij} \\sim \\text{Bernoulli}(1-p)$. При этом при каждом новом проходе вперед генерируется новая маска (то есть она не фиксирована). Функция слоя (умножение поэлементное):\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1-p} m * x\n",
    "$$\n",
    "\n",
    "Нормализация на $1-p$ необходима, чтобы среднее значение нейронов входа не изменилось.\n",
    "\n",
    "**Режим eval:**\n",
    "\n",
    "Здесь все предельно просто: вход слоя никаких не изменяется $y=x$.\n",
    "\n",
    "**Хозяйке на заметку**\n",
    "\n",
    "- Убедитесь, что проход назад корректно работает и для train, и для eval режимов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "collective-western",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dropout ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_dropout()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "appointed-metropolitan",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Функции активации (1.5 балла)\n",
    "\n",
    "### ReLU\n",
    "\n",
    "- Прототип: [nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "- Расположение: `modules.activations.ReLU`\n",
    "\n",
    "Функция слоя:\n",
    "\n",
    "$$\n",
    "y = \\max(x, 0)\n",
    "$$\n",
    "\n",
    "> -----\n",
    "> $$ \\frac{\\partial l}{\\partial x} = \\frac{\\partial l}{\\partial y} \\frac{\\partial y}{\\partial x} $$\n",
    "> $$ \\frac{\\partial y}{\\partial x} = \\begin{cases} 1, x > 0 \\\\ 0, x \\leq 0  \\end{cases} $$\n",
    ">-----\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "- Прототип: [nn.Sigmoid](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html?highlight=nn%20sigmoid#torch.nn.Sigmoid)\n",
    "- Расположение: `modules.activations.Sigmoid`\n",
    "\n",
    "Функция слоя:\n",
    "\n",
    "$$\n",
    "y = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    ">-------\n",
    "> $$ \\frac{\\partial y}{\\partial x} = \\sigma(x)(1 - \\sigma(x)) $$\n",
    ">-------\n",
    "\n",
    "### Softmax\n",
    "\n",
    "- Прототип: [nn.Softmax](http://bit.ly/get3a)\n",
    "- Расположение: `modules.activations.Softmax`\n",
    "\n",
    "Функция слоя:\n",
    "\n",
    "$$\n",
    "y_{ij} = \\frac{\\exp(x_{ij})}{\\sum_{k=1}^{N} \\exp(x_{ik})}\n",
    "$$\n",
    "\n",
    ">-----\n",
    "> $$ \\frac{\\partial y_{ij}}{\\partial x_{im}} = \\begin{cases} softmax(x_{im})(1 - softmax(x_{im})), \\ m = j \\\\ -softmax(x_{ij})softmax(x_{im}), \\ m \\neq j  \\end{cases} \\Rightarrow softmax(x_{ij})([m = j] - softmax(x_{im})) $$\n",
    "> $$ \\frac{\\partial l}{\\partial x_{ik}} = \\sum_{j=1}^{N} \\frac{\\partial l}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial x_{ik}} = \\sum_{j=1}^{N} grad\\_output_{ij} softmax(x_{ik})([k = j] - softmax(x_{ij}))  $$\n",
    "> $$ = softmax(x_{ik}) \\sum_{j=1}^{N} \\underbrace{grad\\_output_{ij}[k = j]}_{=0, \\ k \\neq j} - \\sum_{j=1}^{N} grad\\_output_{ij}[k \\neq j] softmax(x_{ij}) $$\n",
    "> $$ = softmax(x_{ik}) (grad\\_output_{ik} - \\sum_{j=1}^{N} grad\\_output_{ij}[k \\neq j] softmax(x_{ij})) $$\n",
    ">----\n",
    "\n",
    "### LogSoftmax\n",
    "\n",
    "- Прототип: [nn.LogSoftmax](https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html?highlight=log%20softmax#torch.nn.LogSoftmax)\n",
    "- Расположение: `modules.activations.LogSoftmax`\n",
    "\n",
    "Функция слоя:\n",
    "\n",
    "$$\n",
    "y_{ij} = \\log \\left(\\frac{\\exp(x_{ij})}{\\sum_{k=1}^{N} \\exp(x_{ik})}\\right)\n",
    "$$\n",
    "\n",
    ">-------\n",
    "> $$ \\log \\left(\\frac{\\exp(x_{ij})}{\\sum_{k=1}^{N} \\exp(x_{ik})}\\right) = \\log(exp(x_{ij})) - \\log(\\sum_{k=1}^{N} \\exp(x_{ik})) = x_{ij} - \\text{scipy.logsumexp}(x_i) $$\n",
    "> $$ \\frac{\\partial y_{ij}}{\\partial x_{im}} = \\begin{cases} 1 - \\frac{exp(x_{ij})}{\\sum_{k=1}^{N} \\exp(x_{ik})}, \\ m = j \\\\ -\\frac{exp(x_{im})}{\\sum_{k=1}^{N} \\exp(x_{ik})}, \\ m \\neq j \\end{cases} $$\n",
    "> $$ \\frac{\\partial l}{\\partial x_{im}} = \\sum_{j=1}^{N} \\frac{\\partial l}{\\partial y_{ij}} \\frac{\\partial y_{ij}}{\\partial x_{im}} = \\sum_{j=1}^{N} grad\\_output_{ij} ([m=j] - softmax(x_{im})) = grad\\_output_{im} - softmax(x_{im}) \\sum_{j=1}^{N} grad\\_output_{ij} $$\n",
    ">-----\n",
    "\n",
    "**Хозяйке на заметку**\n",
    "\n",
    "- Пользуйтесь функциями из `scipy.special`\n",
    "- Реализовывать `LogSoftmax` как логарифм от модуля `Softmax` &mdash; плохая идея"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "great-sector",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_activations ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_activations()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "disabled-proof",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Контейнер Sequential (1 балл)\n",
    "\n",
    "- Прототип: [nn.Sequential](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html)\n",
    "- Расположение: `modules.layers.Sequential`\n",
    "\n",
    "Контейнер-обертка, который применяет слои последовательно.\n",
    "\n",
    "**Важно:** здесь разрешен цикл по модулям при проходах вперед и назад."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "standard-electron",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sequential ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_sequential()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "extensive-mirror",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 6. Функции потерь (1 балл)\n",
    "\n",
    "Функции потерь отличаются от всех остальных модулей тем, что являются стоком вычислительного графа (то есть из них нет исходящих операций). Это означает, что с них начинается проход назад, поэтому интерфейс функции `compute_grad_input` выглядит иначе: вместо входа модуля и производной по выходу в функцию приходит предсказание нейронной сети (производная по которому нас и интересует, чтобы запустить проход назад) и целевая переменная. Базовый класс для всех функций потерь &mdash; `modules.base.Criterion`.\n",
    "\n",
    "### MSE\n",
    "\n",
    "- Прототип: [nn.MSELoss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss)\n",
    "- Расположение: `modules.criterions.MSELoss`\n",
    "\n",
    "Пусть $f \\in \\mathbb{R}^{B\\times N}$ &mdash; предсказание нейронной сети, а $y \\in \\mathbb{R}^{B\\times N}$ &mdash; целевая переменная. Функция потерь выглядит как:\n",
    "\n",
    "$$\n",
    "\\ell(f, y) = \\frac{1}{BN} \\sum_{i=1}^B \\sum_{j=1}^N (f_{ij} - y_{ij})^2\n",
    "$$\n",
    "\n",
    "> $$ \\nabla_{f} \\ell(f, y) = \\frac{2}{BN} (f_{ij} - y_{ij}) $$\n",
    "\n",
    "### Cross Entropy\n",
    "\n",
    "- Прототип: [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)\n",
    "- Расположение: `modules.criterions.CrossEntropyLoss`\n",
    "\n",
    "Кросс-энтропия &mdash; это функция потерь для обучения классификаторов. Пусть $f \\in \\mathbb{R}^{B\\times C}$ (где $C$ &mdash; число классов) &mdash; предсказание нейронной сети (это так называемые *логиты*, обычно выходы линейного слоя без активации, потому могут быть любыми вещественными числами), а $y \\in \\{1, \\dots, C\\}^B$ &mdash; целевая переменная (номер класса соответствующего объекта). Функция потерь вычисляется как:\n",
    "\n",
    "$$\n",
    "p_{ic} = \\frac{\\exp(f_{ic})}{\\sum_{k=1}^C \\exp(f_{ik})}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ell(f, y) = -\\frac{1}{B} \\sum_{i=1}^B \\sum_{c=1}^C [c = y_i] \\log p_{ic}\n",
    "$$\n",
    "\n",
    "При этом, $p_{ic}$ &mdash; это вероятность класса $с$ для объекта $i$, которую предсказывает нейронная сеть.\n",
    "\n",
    "**Важно:** вычисление Softmax, а затем логарифма от него &mdash; численно нестабильная операция. Воспользуйтесь слоем LogSoftmax.\n",
    "\n",
    "> $$ \\nabla_{f_{ic}} \\ell(f, y) = -\\frac{ \\frac{\\partial p_{ic}}{\\partial f_{ic}}}{B p_{ic}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "early-hampshire",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_criterions ... OK\n"
     ]
    }
   ],
   "source": [
    "import tests\n",
    "\n",
    "tests.test_criterions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "steady-edition",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 7. Оптимизаторы (1.5 балла)\n",
    "\n",
    "Оптимизатор &mdash; вспомогательный класс, который обновляет веса нейронной сети при градиентном спуске, используя сохраненные градиенты параметров. Базовый класс &mdash; `modules.base.Optimizer`. В документации PyTorch приведен псевдокод с описанием алгоритмов, советуем обратиться туда.\n",
    "\n",
    "**Важно:** здесь разрешен цикл по параметрам и градиентам (см. шаблоны)\n",
    "\n",
    "### SGD\n",
    "\n",
    "- Прототип: [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)\n",
    "- Расположение: `modules.optimizers.SGD`\n",
    "\n",
    "> $$ g_t = g_t + \\lambda \\theta_{t-1} $$\n",
    "> $$ b_t = \\mu b_{t-1} + g_t $$\n",
    "> $$ g_t = b_t $$\n",
    "> $$ \\theta_{t} = \\theta_{t-1} - \\gamma g_t $$\n",
    "\n",
    "### Adam\n",
    "\n",
    "- Прототип: [torch.optim.Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html#torch.optim.Adam)\n",
    "- Расположение: `modules.optimizers.Adam`\n",
    "\n",
    "> $$ g_t = g_t + \\lambda \\theta_{t-1} $$\n",
    "> $$ m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t $$\n",
    "> $$ v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2 $$\n",
    "> $$ \\widehat m_t = \\frac{m_t}{1 - \\beta_1^t} $$\n",
    "> $$ \\widehat v_t = \\frac{v_t}{1 - \\beta_2^t} $$\n",
    "> $$ \\theta_{t} = \\theta_{t-1} - \\gamma \\frac{\\widehat m_t}{\\sqrt{\\widehat v_t} + \\varepsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "selected-cuisine",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_optimizers ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_optimizers()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "centered-therapist",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 8. DataLoader (0.5 балла)\n",
    "\n",
    "- Прототип: [torch.utils.data.DataLoader](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)\n",
    "- Расположение: `modules.dataloader.DataLoader`\n",
    "\n",
    "И последнее, что нам осталось реализовать &mdash; это DataLoader, который перемешивает данные раз в эпоху (если это необходимо) и формирует из них мини-батчи. Технически, это будет питоновский итератор. Вот краткое [руководство](https://stackoverflow.com/questions/19151/how-to-build-a-basic-iterator), как написать итератор.\n",
    "\n",
    "Обратите внимание, что ваша реализация должна уметь работать как с одномерным массивом целевой переменной (с формой `(num_samples, )` &mdash; так будет удобнее учить нейронную сеть на кросс-энтропию), так и с двумерной версией (с формой`(num_samples, 1)` &mdash; сответственно, на MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ready-trance",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_dataloader ... OK\n"
     ]
    }
   ],
   "source": [
    "tests.test_dataloader()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "active-algorithm",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Собираем все вместе\n",
    "\n",
    "Если вы все сделали правильно, то следующий кусок кода с обучением нейронной сети должен заработать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "innovative-fetish",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import modules as mm\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "super-latest",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X_train = np.random.randn(2048, 8)\n",
    "X_test = np.random.randn(512, 8)\n",
    "y_train = np.sin(X_train).sum(axis=1, keepdims=True)\n",
    "y_test = np.sin(X_test).sum(axis=1, keepdims=True)\n",
    "\n",
    "train_loader = mm.DataLoader(X_train, y_train, batch_size=64, shuffle=True)\n",
    "test_loader = mm.DataLoader(X_test, y_test, batch_size=64, shuffle=False)\n",
    "\n",
    "model = mm.Sequential(\n",
    "    mm.Linear(8, 32),\n",
    "    mm.BatchNormalization(32),\n",
    "    mm.ReLU(),\n",
    "    mm.Linear(32, 64),\n",
    "    mm.Dropout(0.25),\n",
    "    mm.Sigmoid(),\n",
    "    mm.Linear(64, 1)\n",
    ")\n",
    "optimizer = mm.Adam(model, lr=1e-2)\n",
    "criterion = mm.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "gothic-latin",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6d7c708d124424c8c532858d971d9bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "pbar = tqdm(range(1, num_epochs + 1))\n",
    "\n",
    "for epoch in pbar:\n",
    "    train_loss, test_loss = 0.0, 0.0\n",
    "\n",
    "    model.train()\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        model.backward(X_batch, criterion.backward(predictions, y_batch))\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss * X_batch.shape[0]\n",
    "\n",
    "    model.eval()\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        predictions = model(X_batch)\n",
    "        loss = criterion(predictions, y_batch)\n",
    "        test_loss += loss * X_batch.shape[0]\n",
    "\n",
    "    train_loss /= train_loader.num_samples()\n",
    "    test_loss /= test_loader.num_samples()\n",
    "    pbar.set_postfix({'train loss': train_loss, 'test loss': test_loss})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
